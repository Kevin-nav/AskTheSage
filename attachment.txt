
### Full Ruleset for Gauging MCQ Difficulty

Here is the complete ruleset for gauging the difficulty of multiple-choice questions (MCQs) for the courses, combining the core principles with the specific considerations for the MCQ format.

---

### **Core Principle**

The final difficulty score is a product of multiple factors. You'll start with a **Base Score** based on the cognitive demand of the question and then apply a series of **Multipliers** that account for the complexity of the notes, the question's structure, and the quality of the distractors.

**Difficulty Score = Base Score × Multiplier₁ × Multiplier₂ × ...**

You can scale the final score to a simple range, like 1-5 or 1-10, for easy interpretation.

---

### **The Rule Set**

#### **1. Base Score (Cognitive Demand)**

This is the foundational score based on the level of thinking required to answer the question, using a simplified version of Bloom's Taxonomy.

* **Remembering (Base Score = 1.0):** The question asks for the recall of specific facts, definitions, or terms directly from the notes.
* **Understanding (Base Score = 1.2):** The student must explain a concept in their own words or summarize information.
* **Applying (Base Score = 1.5):** The question requires the use of a formula, principle, or method to solve a problem. These are typically calculation-based.
* **Analyzing (Base Score = 2.0):** The student must break down a concept, compare different ideas, or identify relationships between components.

---

#### **2. Multipliers (Note Reference)**

This multiplier assesses the difficulty of finding the necessary information within the reference notes.

* **Single, Direct Reference (Multiplier = 1.0):** The answer is a single, isolated piece of information in one location (e.g., a single sentence or a formula).
* **Multiple Dispersed References (Multiplier = 1.2):** The answer requires the student to combine information from two or more distinct sections of the notes.
* **Conceptual/Implied Reference (Multiplier = 1.5):** The answer is not explicitly stated but must be inferred by connecting multiple concepts.

---

#### **3. Multipliers (Distractor Complexity)**

This is a crucial multiplier unique to the MCQ format. It evaluates the quality and plausibility of the incorrect answer choices.

* **Simple Distractors (Multiplier = 1.0):** The incorrect options are obviously wrong or wildly unrelated to the topic.
* **Plausible Distractors (Multiplier = 1.2):** The distractors are plausible incorrect answers. This often happens when the distractors are the result of common student mistakes (e.g., using the wrong formula or making a simple calculation error).
* **Misconception-Based Distractors (Multiplier = 1.5):** The distractors are specifically designed to reveal a deep-seated student misconception. The student cannot get the right answer without truly understanding the concept.

---

#### **4. Multipliers (Question Structure and Math)**

This category combines the structure of the question stem and the complexity of any required calculations or diagrams.

* **Single-Step Question (Multiplier = 1.0):** The question requires a single piece of information or a single calculation to find the answer.
* **Multi-Step Question (Multiplier = 1.3):** The question or the process to find the correct answer involves multiple steps.
* **Complex Algebra (Multiplier = 1.3):** The question requires rearranging formulas, solving multi-step equations, or working with systems of equations.
* **Graphical or Diagrammatic Interpretation (Multiplier = 1.5):** The student must read, interpret, or create a graph or circuit diagram to solve the problem.

This comprehensive set of rules should provide a consistent and reliable method for scoring the difficulty of your MCQs based on your course notes.







╭─────────────────────────────────────────────────────────────────────────────────────────╮
│  > what do you think of this method of selecting questions for users? @json-sample.txt  │
╰─────────────────────────────────────────────────────────────────────────────────────────╯

 ╭────────────────────────────────────────────────────────────────────────────────────────────────────────╮
 │ ✔ ReadManyFiles Will attempt to read and concatenate files using patterns:                            │
 │    json-sample.txt                                                                                     │
 │     (within target directory:                                                                          │
 │    C:\Users\MORO\Project\other_projects\Johnson_Bot                                                    │
 │    ) . Excluding: patterns like                                                                        │
 │    **/node_modules/**`, `**/.git/**...`. File encoding: utf-8. Separator: "--- path/to/file.ext ---".  │
 │                                                                                                        │
 │    ### ReadManyFiles Result (Target Dir: `C:\Users\MORO\Project\other_projects\Johnson_Bot`)           │
 │                                                                                                        │
 │    Successfully read and concatenated content from **1 file(s)**.                                      │
 │                                                                                                        │
 │    **Processed Files:**                                                                                │
 │    - `json-sample.txt`                                                                                 │
 ╰────────────────────────────────────────────────────────────────────────────────────────────────────────╯
✦ This is an exceptionally well-thought-out and comprehensive plan for an adaptive learning system. It moves
  from a high-level concept to a detailed, practical, and robust implementation guide.

  Here's my assessment of the approach:

  Key Strengths:

   1. Excellent Core Algorithm: The final UniversalQuestionSelector is the strongest part of the design. By
      focusing on user performance (weakness, newness, spaced repetition) rather than a manually-assigned,
      subject-specific "difficulty," the system becomes truly universal and scalable. It correctly identifies
      that a question's difficulty is relative to the user.
   2. Strong Architecture: The separation of concerns into three distinct layers (UniversalQuestionSelector for
      pure logic, DatabaseQuestionSelector for data interaction, and AdaptiveQuizService for business logic) is
      a best practice that will make the system easier to maintain, test, and extend.
   3. Data-Driven & Future-Proof: The creation of the interaction_logs table is crucial. It captures the
      necessary data not just for analytics but for future machine learning models that can further refine the
      selection process without requiring architectural changes.
   4. Practical and Actionable: The plan doesn't just stay at a theoretical level. It provides concrete SQL for
      database migrations, full Python class implementations, and a clear setup guide, making it immediately
      actionable.
   5. Intelligent Distribution Control: The system doesn't just pick the highest-scoring questions. The
      _apply_distribution_control method is smart because it ensures a healthy mix of question types (weakness,
      new, SRS). This prevents a user from getting stuck on 100% weakness-based questions, which can be
      demotivating.

  Points of Consideration:

   * Evolution of the Idea: The document shows a positive evolution. It initially proposes a complex, manual
     difficulty scoring system but then pivots to the far superior universal algorithm. The universal approach
     is the right choice as it avoids the immense overhead of manually rating every question in every subject.
   * Database Performance: The query to fetch a user's performance history (_get_user_performance_history) is
     necessarily complex. As noted in the setup guide, ensuring you have the proper database indexes
     (idx_user_answers_user_question, etc.) will be absolutely critical to ensure the system remains fast as
     the amount of data grows.
   * New User Experience: The "cold start" problem for new users is handled by random selection. This is a
     solid baseline. In the future, you could enhance this by prioritizing questions that have a high global
     success rate across all users, ensuring a smoother onboarding experience.

  Overall Opinion:

  This is an A+ design. It's a powerful, flexible, and well-engineered foundation for a truly personalized
  learning bot. By focusing on universal learning patterns and building in robust data collection from day
  one, you are setting the project up for long-term success. The proposed system will be a significant
  upgrade from a simple random question generator, turning the bot into a genuine learning companion.

╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > /plan how can we then turn this idea into reality and what do we need to do to make it a reality, no   │
│    coding yet, we are planing here. think hard and smart here                                             │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ Here is the strategic plan for implementing the adaptive learning system.

  1. Understanding the Goal

  The primary objective is to transition the conceptual adaptive learning algorithm into a fully integrated,
  operational feature within the existing Johnson_Bot application. This involves implementing the database
  schema changes, integrating the new question selection logic, and ensuring the entire system is robust,
  testable, and maintainable, without disrupting existing functionality. The focus is on strategic planning
  before any implementation begins.

  2. Investigation & Analysis

  Before planning the implementation, a thorough investigation of the current codebase is required to
  understand the existing architecture and identify the precise integration points.

  Critical Questions to Answer:

   1. Which codebase is active? The project contains multiple application directories (src,
      Sir_Johnson_study_bot/src, Sir_Johnson_study_bot/newbot). It's critical to identify the single, canonical
      source tree that is currently deployed and running the bot.
   2. What is the current database schema? The new feature requires significant schema changes. We must know
      the exact structure of the existing questions, users, and any answer-tracking tables.
   3. How are quizzes currently managed? We need to locate the code responsible for starting a quiz, selecting
      questions, processing answers, and calculating scores.
   4. How are database sessions and migrations handled? Understanding the existing database connection and
      migration (Alembic) patterns is essential for introducing schema changes safely.

  Investigative Steps:

   * Step 1: Identify the Active Codebase & Dependencies
       * Read requirements.txt to confirm the presence and versions of key libraries like SQLAlchemy, alembic,
         and python-telegram-bot.
       * Analyze alembic.ini and the top-level src directory structure. The presence of a mature structure
         like this strongly suggests it is the primary, active codebase over the nested Sir_Johnson_study_bot
         directories. This will be the target for all modifications.

   * Step 2: Analyze the Database Schema
       * Read the models.py file located at src/models/models.py to get a definitive definition of the current
         database tables, columns, and relationships.
       * List the contents of the alembic/versions/ directory to understand the history of schema changes.
         This confirms how tables like quiz_sessions and questions have evolved.

   * Step 3: Locate the Core Quiz Logic
       * Search the codebase for key terms like start_quiz, get_random_question, and process_answer.
       * Read the contents of src/services/quiz_service.py. This file is the most likely candidate for
         containing the core business logic for quiz management.
       * Read src/handlers/conversation_handlers.py to understand how user interactions (Telegram commands and
         button clicks) are mapped to the quiz_service.

  3. Proposed Strategic Approach

  The implementation will be broken down into distinct, sequential phases to manage complexity and minimize
  risk.

  Phase 0: Project Scaffolding & Isolation

   1. Create a New Module: Inside the src directory, create a new package named adaptive_learning.
   2. Isolate Logic: Place the pure, universal algorithm code (UniversalQuestionSelector) into
      src/adaptive_learning/selector.py. Place the database integration classes (DatabaseQuestionSelector,
      AdaptiveQuizService) into src/adaptive_learning/service.py.
   3. Add Models: Define the new InteractionLog table in src/models/models.py.

  Outcome: The new logic is added to the codebase in an organized and isolated manner without being
  activated yet.

  Phase 1: Database Migration (The Foundation)

   1. Generate Migration Script: Use the alembic revision --autogenerate command to create a new migration
      file.
   2. Add Schema Changes: Manually add the ALTER TABLE and CREATE TABLE statements from the design document
      into the generated script. This includes adding columns to questions, user_answers, quiz_sessions, and
      creating the interaction_logs table.
   3. Apply Migration: Run alembic upgrade head to apply the changes to the database.

  Outcome: The database schema is updated to support all necessary features for the adaptive algorithm. This
  is the most critical prerequisite.

  Phase 2: Service Layer Integration

   1. Introduce a Feature Flag: In the project's configuration, add a flag, e.g., ADAPTIVE_QUIZ_ENABLED = 
      True/False.
   2. Modify Existing Quiz Service: In src/services/quiz_service.py, modify the methods that start quizzes and
      select questions.
   3. Conditional Logic: Use the feature flag. If ADAPTIVE_QUIZ_ENABLED is true, the method will instantiate
      and call the new AdaptiveQuizService. If false, it will execute the old logic (e.g., random question
      selection). This ensures zero disruption to existing users and allows for safe testing.
   4. Update Answer Processing: Similarly, update the answer submission logic to call the new service, which
      will handle the comprehensive logging to user_answers and interaction_logs.

  Outcome: The new adaptive logic is wired into the application's service layer, controlled by a feature
  flag for safe activation.

  Phase 3: Finalizing and Enabling

   1. Configuration: Add the ADAPTIVE_QUIZ_CONFIG dictionary to the project's configuration file so that
      algorithm parameters can be tuned without code changes.
   2. Testing and Rollout: After thorough verification (see next section), enable the feature flag by default
      for all users or roll it out incrementally (e.g., for specific courses or user IDs).

  Outcome: The adaptive learning system is fully implemented, configurable, and live.

  4. Verification Strategy

  A multi-layered testing approach is required to ensure the new system works as expected and does not
  introduce regressions.

   1. Unit Testing:
       * Write dedicated tests for the UniversalQuestionSelector in src/adaptive_learning/selector.py. These
         tests will not touch the database and will verify that given a specific user history, the scoring and
         selection logic produces the correct output and distribution.

   2. Integration Testing:
       * Write tests for the AdaptiveQuizService that connect to a dedicated test database.
       * These tests will verify that:
           * Starting a quiz correctly creates a QuizSession and populates QuizSessionQuestion with an
             appropriate mix of questions.
           * Submitting an answer correctly updates all relevant tables (user_answers, interaction_logs,
             etc.).
           * A full quiz lifecycle from start to finish works as expected.

   3. End-to-End Testing:
       * In a staging environment, perform manual testing using a Telegram client.
       * Test the complete user flow for both new users and existing users with a known history.
       * After each test run, query the database directly to validate that the data in all affected tables is
         correct.

   4. Regression Testing:
       * Run the entire existing test suite with the feature flag turned False to ensure no part of the old
         functionality was broken.
       * Run the entire test suite again with the feature flag turned True.

  5. Anticipated Challenges & Considerations

   * Data Backfilling: The algorithm's effectiveness for existing users depends on historical data
     (correct_streak, next_review_date). A one-time data migration script should be written and run to analyze
     the existing user_answers table and populate these new fields for all historical records.
   * Performance Under Load: The database query for fetching user performance history is complex. For a large
     user base, this query could become a bottleneck. It should be benchmarked, and if necessary, optimized or
     cached.
   * First User Experience: The default strategy for new users (random selection) is acceptable but not ideal.
     A potential enhancement is to prioritize questions marked as "introductory" or those with a high global
     success rate.
   * Transaction Management: The process of starting a quiz and processing an answer now involves multiple
     database writes. These operations must be wrapped in a single database transaction to ensure data
     integrity. If any step fails, the entire operation should be rolled back.

╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > we are using the main folder the Sir_Johnson_study_bot/ was just an attchment and it is not the main so  │
│    you can just ignore it. Also, I like the fact that you spoke about first user experience is good, the    │
│    current experience is not ideal in any way. let' see how we can make it better even thhough we do not    │
│    have                                                                                                     │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯


╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > we are using the main folder the Sir_Johnson_study_bot/ was just an attchment and it is not the main so  │
│    you can just ignore it. Also, I like the fact that you spoke about first user experience is good, the    │
│    current experience is not ideal in any way. let' see how we can make it better even thhough we do not    │
│    have ## Simple Summary: Adaptive Quiz Bot Algorit                                                        │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯


╭─────────────╮
│  > 1. **60  │
╰─────────────╯


╭───────────────────────────╮
│  > 3. **15% Review Quest  │
╰───────────────────────────╯


╭─────────────╮
│  > 1. **60  │
╰─────────────╯


╭───────────────────────────╮
│  > 3. **15% Review Quest  │
╰───────────────────────────╯


╭──────────────────────╮
│  > Question Score =  │
╰──────────────────────╯


ℹRequest cancelled.
 
✕ [API Error: Cannot read properties of undefined (reading 'error')]


╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > we are using the main folder the Sir_Johnson_study_bot/ was just an attchment and it is not the main so  │
│    you can just ignore it. Also, I like the fact that you spoke about first user experience is good, the    │
│    current experience is not ideal in any way. let' see how we can make it better even thhough we do not    │
│    have data to help it be the best yet. so that a suer suing at first would stll enjoy it.                 │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ Excellent point. You've identified the "coldest start" problem: How do we provide a good experience when the
  bot is first launched and has zero performance data to rely on? My previous strategy depended on that data
  existing.

  You are correct, we need a plan for Day 1, Hour 1. Let's devise a strategy that works even before a single
  quiz has been taken by any user.

  1. Understanding the Goal

  The objective is to design a "cold start" strategy for a new user's first quiz in a course where no
  historical performance data exists. The strategy must use heuristics based on the intrinsic properties of
  the questions themselves to create a more welcoming experience than simple random selection.

  2. Investigation & Analysis

  Since we have no performance data, we must look at the question's structure itself. The key is to find a
  piece of metadata that can act as a proxy for complexity.

  Critical Questions & Findings:

   1. What metadata do we have on the `Question` model?
       * A detailed investigation of the database schema (from src/models/models.py and alembic migrations
         like 4bd039fb843b...) reveals a critical boolean field: has_latex.
   2. What does `has_latex` imply?
       * The presence of LaTeX in a question strongly implies that it contains mathematical formulas,
         equations, or other complex scientific notation.
       * Hypothesis: Questions without LaTeX are more likely to be definition-based, conceptual, or
         vocabulary-related. Questions with LaTeX are more likely to require calculation or formula
         interpretation. Therefore, has_latex can serve as a reasonable, objective proxy for a certain type of
         complexity.

  3. Proposed Strategic Approach

  I propose a "Foundations First" strategy for the absolute "cold start" scenario. This approach prioritizes
  conceptual questions before introducing ones that require complex notation or calculation.

  Phase 1: The "Foundations First" Selection Algorithm

  This algorithm is used only when the system detects that there is not enough global performance data to
  use the more advanced "Balanced Portfolio" strategy.

   1. Separate Questions into Two Pools: When a new user starts a quiz in a "cold" course, the selection logic
      will first query all questions for that course and divide them into two distinct pools:
       * Pool A (Conceptual): All questions where has_latex = False.
       * Pool B (Technical/Formulaic): All questions where has_latex = True.

   2. Construct a "Ramped" Quiz: The quiz will be assembled by drawing from these pools in a specific order to
      create a smooth ramp in complexity. For a 20-question quiz:
       * First 75% of the quiz (e.g., 15 questions): Randomly select questions exclusively from Pool A 
         (Conceptual). This ensures the user begins with questions that are less likely to require complex
         calculations, allowing them to engage with the core concepts first.
       * Final 25% of the quiz (e.g., 5 questions): Randomly select questions from Pool B 
         (Technical/Formulaic). This introduces the more notation-heavy questions towards the end of the quiz.

  Outcome: The user's first experience is structured and logical. It builds from foundational concepts to
  more complex applications, which is a much better pedagogical approach than a completely random mix.

  Phase 2: The Hybrid System - Combining Strategies

  The "Foundations First" strategy is a starting point, not the final destination. The superior, data-driven
  "Balanced Portfolio" strategy should be used as soon as possible. The system will therefore be a hybrid:

   1. Define a Data Sufficiency Threshold: In the configuration, set a threshold for what it means to have
      "enough" data. For example: DATA_IS_SUFFICIENT_WHEN_PROVEN_QUESTIONS >= 30.
   2. Implement a Master Selector Logic: The start_quiz function for a new user will now follow this master
      logic:

   1     IF (number of questions with >10 attempts >= DATA_IS_SUFFICIENT_WHEN_PROVEN_QUESTIONS) THEN
   2         // Use the superior, data-driven strategy
   3         select_quiz_using_balanced_portfolio()
   4     ELSE
   5         // Fall back to the heuristic-based strategy for cold starts
   6         select_quiz_using_foundations_first()
   7     END IF

  Outcome: The bot automatically "graduates" from the heuristic-based "Foundations First" strategy to the
  more accurate, data-driven "Balanced Portfolio" strategy on a course-by-course basis as soon as it
  collects enough user data.

  4. Verification Strategy

   1. Test the "Cold Start": Create a test case with a database where all questions have total_attempts = 0.
      Assert that the quiz served to a new user is composed of ~75% non-LaTeX and ~25% LaTeX questions.
   2. Test the "Warm Start": Create a test case where the database has sufficient global performance data.
      Assert that the system ignores the has_latex flag and instead serves a quiz based on the "Balanced
      Portfolio" (global success rate) logic.
   3. Test the Transition: Design a test that simulates the system collecting data, and verify that the
      selection strategy correctly switches from "Foundations First" to "Balanced Portfolio" once the data
      sufficiency threshold is met.

  5. Anticipated Challenges & Considerations

   * The `has_latex` Assumption: This strategy's effectiveness rests on the assumption that has_latex is a
     good proxy for a certain type of complexity. This holds true for many technical subjects but may be less
     relevant for others (e.g., French, History). However, in the complete absence of any other data, it is
     the most logical and objective starting point available.
   * Data Integrity: The process that imports questions into the database must be robust and correctly set the
     has_latex flag for this strategy to work at all.

╭───────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > let's also talk about the difficulty level metric, let me explain how it works and how we can use it   │
│    for the cold start. @attachment.txt                                                                    │
╰───────────────────────────────────────────────────────────────────────────────────────────────────────────╯

 ╭────────────────────────────────────────────────────────────────────────────────────────────────────────╮
 │ ✔ ReadManyFiles Will attempt to read and concatenate files using patterns:                            │
 │    attachment.txt                                                                                      │
 │     (within target directory:                                                                          │
 │    C:\Users\MORO\Project\other_projects\Johnson_Bot                                                    │
 │    ) . Excluding: patterns like                                                                        │
 │    **/node_modules/**`, `**/.git/**...`. File encoding: utf-8. Separator: "--- path/to/file.ext ---".  │
 │                                                                                                        │
 │    ### ReadManyFiles Result (Target Dir: `C:\Users\MORO\Project\other_projects\Johnson_Bot`)           │
 │                                                                                                        │
 │    Successfully read and concatenated content from **1 file(s)**.                                      │
 │                                                                                                        │
 │    **Processed Files:**                                                                                │
 │    - `attachment.txt`                                                                                  │
 ╰────────────────────────────────────────────────────────────────────────────────────────────────────────╯
✦ Thank you for providing the detailed ruleset. This is a game-changer for the "cold start" problem. Having a
  pre-defined, quantitative difficulty_score is a significant asset that allows us to create a much more
  intelligent first-quiz experience than heuristics alone.

  Here is a revised strategic plan that leverages this difficulty_score for the initial user experience.

  1. Understanding the Goal

  The objective is to design a "cold start" quiz strategy that uses the provided manual difficulty_score to
  create a structured, pedagogically-sound first quiz for new users. This "Guided Placement" quiz will serve
  as a superior alternative to random selection when no real user performance data is available.

  2. Investigation & Analysis

  The new information requires us to confirm that our system can support this difficulty_score.

  Critical Questions & Findings:

   1. Does the database schema support this?
       * Yes. A review of the project's migration history (4bd039fb843b...) and the Question model in
         src/models/models.py confirms that a difficulty_score column already exists. The ruleset implies this
         should be a Float or Decimal type to handle the multiplicative results.

   2. What is the main prerequisite for this strategy?
       * The single most important prerequisite is that the difficulty_score for each question must be
         calculated according to your ruleset and populated in the database. This strategy is entirely
         dependent on that data being present.

  3. Proposed Strategic Approach

  We will implement a "Guided Placement" strategy. This approach uses the expert-defined difficulty_score to
  create a quiz that intentionally ramps up in difficulty, providing a smooth and logical onboarding for new
  users.

  Phase 1: Data Population (Prerequisite)

   1. Establish a Scoring Process: Before this strategy can be effective, a process must be implemented for
      scoring questions. This is a content-creation task. For every question added to the system, its
      difficulty_score must be calculated using your ruleset and saved to the database.
   2. Perform a Backfill: For all existing questions in the database, a one-time script should be run to
      calculate and backfill their difficulty_score.

  Outcome: The database is populated with the expert-driven data needed to power the "Guided Placement"
  quiz.

  Phase 2: The "Guided Placement" Selection Algorithm

  This algorithm is used only for a user's first quiz in a course, and only when that course lacks
  sufficient global performance data.

   1. Define Difficulty Bands: In the AdaptiveQuizService, create a system to categorize questions into bands
      based on their difficulty_score. These thresholds should be configurable.
       * Band 1 (Easy): difficulty_score <= 1.5
       * Band 2 (Medium): 1.5 < difficulty_score <= 3.0
       * Band 3 (Hard): difficulty_score > 3.0

   2. Construct the Ramped Quiz: The algorithm will build the quiz by selecting questions from these bands in a
      specific order. For a 20-question quiz:
       * Questions 1-5 (Warm-up): Randomly select 5 questions from Band 1 (Easy).
       * Questions 6-15 (Core Assessment): Randomly select 10 questions from Band 2 (Medium).
       * Questions 16-20 (Challenge): Randomly select 5 questions from Band 3 (Hard).
   3. Maintain Order: Unlike other strategies, the final list of questions should not be shuffled. The explicit
      goal is to create a smooth ramp in difficulty from easy to hard.

  Outcome: A new user receives a thoughtfully structured quiz that builds in intensity, providing a much
  better pedagogical experience than a random assortment of questions.

  Phase 3: The Full Hybrid System

  This "Guided Placement" strategy is the new baseline for when no other data is available. The full
  hierarchy of strategies is now:

   1. Personalized (Warmest Start): If a user has a personal history in the course, the full adaptive algorithm
      is used.
   2. Global Performance (Warm Start): If the user is new but the course has sufficient global performance
      data, the "Balanced Portfolio" strategy (based on global success rates) is used. Real user data, when
      available, is a more accurate measure of difficulty than any manual system.
   3. Guided Placement (Cold Start): If the user is new and the course has insufficient global data, this new
      "Guided Placement" strategy (based on the manual difficulty_score) is used.
   4. Random Selection (Coldest Start): If the difficulty_score is also missing for the questions, the system
      falls back to the simplest random selection.

  This creates a robust, multi-layered system that always uses the best available data to select questions.

  4. Verification Strategy

   1. Test the "Guided Placement" Logic: Create a test case with a database of questions having pre-defined
      difficulty_score values. Assert that the quiz served to a new user is ordered correctly by difficulty
      bands (e.g., the first 5 questions all have a score <= 1.5).
   2. Test the Fallback: Create a test case where questions have difficulty_score = NULL. Verify that the
      system gracefully falls back to another strategy (like "Foundations First" based on LaTeX, or simple
      random selection).
   3. Test the Hierarchy: Create a test case to verify that the system correctly chooses the "Balanced
      Portfolio" (global success rate) strategy over this "Guided Placement" strategy when sufficient global
      performance data is available.

  5. Anticipated Challenges & Considerations

   * Manual Workload: The primary challenge of this approach is the manual effort required to score every
     question. This process must be maintained as new questions are added to the system.
   * Scoring Consistency: The ruleset, while structured, has subjective elements. A consistent process or
     training may be needed to ensure different people score questions similarly.
   * The "True" Difficulty: The manually-defined difficulty might not always match how users actually perform.
     This is why it's critical that the system prioritizes real performance data as soon as it's available,
     using this manual score only as a starting point.
